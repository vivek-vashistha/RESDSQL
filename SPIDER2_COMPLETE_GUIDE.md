# Complete Spider2 Setup and Evaluation Guide for RESDSQL

This is a comprehensive guide for setting up, using, and evaluating RESDSQL on the Spider2 dataset. This document consolidates all Spider2-related workflows and can be shared with team members to continue development.

## Table of Contents

1. [Overview](#overview)
2. [Prerequisites](#prerequisites)
3. [Initial Setup](#initial-setup)
4. [Data Preparation](#data-preparation)
5. [Workflow Options](#workflow-options)
6. [Evaluation Methods](#evaluation-methods)
7. [Troubleshooting](#troubleshooting)
8. [File Structure](#file-structure)
9. [References](#references)

---

## Overview

**Spider2** is a new text-to-SQL benchmark that evaluates models on real-world enterprise databases:
- **Spider2-Lite**: 547 examples across BigQuery, Snowflake, and SQLite
- **Spider2-Snow**: 547 examples on Snowflake only

**RESDSQL** supports Spider2 through:
- Database adapters for SQLite, BigQuery, and Snowflake
- Automatic database type detection from instance IDs
- Execution-based evaluation (compares query results, not SQL strings)

### Key Features

- âœ… **SQLite Support**: Full support for local SQLite databases (no cloud setup needed)
- âœ… **Cloud Database Support**: BigQuery and Snowflake with credential management
- âœ… **FastAPI Server**: Interactive question-answering via REST API
- âœ… **Batch Inference**: Full dataset evaluation pipeline
- âœ… **Execution-Based Evaluation**: Compares actual query results with golden results

---

## Prerequisites

### 1. System Requirements

- Python 3.8+
- 16GB+ RAM (for base model)
- CUDA GPU (optional, but recommended for faster inference)

### 2. Install Dependencies

```bash
# Core dependencies (should already be installed)
pip install -r requirements.txt

# Additional Spider2 dependencies
pip install snowflake-connector-python>=3.0.0
pip install google-cloud-bigquery>=3.0.0
pip install pandas>=1.3.0
pip install requests>=2.25.0
```

### 3. Download Spider2 Dataset

**Option A: Clone the Official Repository**

```bash
cd /path/to/your/projects
git clone https://github.com/xlang-ai/Spider2.git
cd Spider2/spider2-lite
```

**Option B: Download Specific Files**

You need:
- `spider2-lite.jsonl` - Questions dataset (548 lines)
- `evaluation_suite/gold/` - Golden evaluation results
  - `spider2lite_eval.jsonl` - Evaluation metadata
  - `exec_result/` - Golden CSV result files (1544 files)
  - `sql/` - Golden SQL queries (optional, for exact match evaluation)
- `resource/databases/sqlite/` - SQLite database files (or download `local_sqlite.zip`)

### 4. Set Up Cloud Database Access (Optional)

**For BigQuery** (if using Spider2-Lite with BigQuery examples):

```bash
# Set up Google Cloud credentials
export GOOGLE_CLOUD_PROJECT="your-project-id"
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account-key.json"
```

**For Snowflake** (if using Spider2-Snow or Spider2-Lite with Snowflake examples):

```bash
export SNOWFLAKE_USER="your-username"
export SNOWFLAKE_PASSWORD="your-password"
export SNOWFLAKE_ACCOUNT="your-account"
export SNOWFLAKE_WAREHOUSE="your-warehouse"
export SNOWFLAKE_DATABASE="your-database"  # Optional
export SNOWFLAKE_SCHEMA="PUBLIC"  # Optional, defaults to PUBLIC
```

**Note**: For SQLite-only evaluation, cloud credentials are not required.

---

## Initial Setup

### Step 1: Organize Your Directory Structure

```
RESDSQL/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ spider2/
â”‚       â”œâ”€â”€ spider2-lite.jsonl          # Questions dataset
â”‚       â”œâ”€â”€ converted/                  # Generated by converter
â”‚       â”‚   â”œâ”€â”€ spider2-lite_tables.json
â”‚       â”‚   â””â”€â”€ spider2-lite_dataset.json
â”‚       â””â”€â”€ README.md
â”œâ”€â”€ database/                           # SQLite databases
â”‚   â”œâ”€â”€ E_commerce/
â”‚   â”‚   â””â”€â”€ E_commerce.sqlite
â”‚   â”œâ”€â”€ Baseball/
â”‚   â”‚   â””â”€â”€ Baseball.sqlite
â”‚   â””â”€â”€ ...
â”œâ”€â”€ predictions/
â”‚   â””â”€â”€ spider2-lite/
â”‚       â””â”€â”€ resdsql_base_natsql/
â”‚           â””â”€â”€ pred.sql
â””â”€â”€ ...
```

### Step 2: Copy Spider2 Data Files

```bash
# Copy the questions dataset
cp /path/to/Spider2/spider2-lite/spider2-lite.jsonl \
   data/spider2/spider2-lite.jsonl

# Extract SQLite databases (if you have local_sqlite.zip)
unzip local_sqlite.zip -d database/
# Or copy from Spider2 repo:
# cp -r /path/to/Spider2/spider2-lite/resource/databases/sqlite/* database/
```

### Step 3: Convert Spider2 Data to RESDSQL Format

```bash
python utils/spider2_converter.py \
    --input data/spider2/spider2-lite.jsonl \
    --output_dir data/spider2/converted \
    --dataset_name spider2-lite
```

This creates:
- `data/spider2/converted/spider2-lite_tables.json` - Schema information (required)
- `data/spider2/converted/spider2-lite_dataset.json` - Converted dataset (optional)

**Verify the conversion:**

```bash
# Check that tables.json was created
ls -lh data/spider2/converted/spider2-lite_tables.json

# Verify it contains database schemas
python -c "import json; data = json.load(open('data/spider2/converted/spider2-lite_tables.json')); print(f'Found {len(data)} databases')"
```

---

## Data Preparation

### Understanding Spider2 Data Format

**spider2-lite.jsonl** contains one JSON object per line:

```json
{
  "instance_id": "local001",
  "db": "E_commerce",
  "question": "How many orders are there?",
  "external_knowledge": "..."
}
```

**Key Fields:**
- `instance_id`: Unique identifier (prefixes: `local*` = SQLite, `bq*` = BigQuery, `sf*` = Snowflake)
- `db`: Database name (used as `db_id` in RESDSQL)
- `question`: Natural language question
- `external_knowledge`: Additional context (optional)

### Database Type Detection

RESDSQL automatically detects database types from `instance_id`:
- `local*` â†’ SQLite (e.g., `local001`, `local002`)
- `bq*` â†’ BigQuery (e.g., `bq001`, `bq002`)
- `sf*` or `snow*` â†’ Snowflake (e.g., `sf001`, `snow001`)

### SQLite Database Setup

SQLite databases should be organized as:
```
database/
â”œâ”€â”€ {db_id}/
â”‚   â””â”€â”€ {db_id}.sqlite
```

Example:
```
database/
â”œâ”€â”€ E_commerce/
â”‚   â””â”€â”€ E_commerce.sqlite
â”œâ”€â”€ Baseball/
â”‚   â””â”€â”€ Baseball.sqlite
```

**Verify databases exist:**

```bash
# Count SQLite databases
find database/ -name "*.sqlite" | wc -l

# List all database IDs
ls -d database/*/ | xargs -n1 basename
```

---

## Workflow Options

You have **two main workflows** for using RESDSQL with Spider2:

### Option 1: FastAPI Server (Recommended for Interactive Use)

**Best for:**
- Testing individual questions
- Interactive development
- Quick prototyping
- One-by-one evaluation

**Advantages:**
- âš¡ **10-25x faster** per question (models cached in memory)
- ðŸ”„ **Interactive**: Ask questions one at a time
- ðŸš€ **No batch processing**: Skip slow preprocessing steps
- ðŸ’¾ **Connection pooling**: Reuses database connections

#### Setup

1. **Start the FastAPI server:**

```bash
bash start_api.sh
# Or: python api_server_refactored.py
```

The server starts on `http://localhost:8000` by default.

2. **Test with a question:**

```bash
curl -X POST "http://localhost:8000/infer" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "How many orders are there?",
    "db_id": "E_commerce",
    "target_type": "natsql",
    "spider2_mode": true,
    "tables_path": "./data/spider2/converted/spider2-lite_tables.json"
  }'
```

3. **Use the interactive API docs:**

Open `http://localhost:8000/docs` in your browser for a Swagger UI interface.

#### Request Parameters

- `question` (required): Natural language question
- `db_id` (required): Database identifier (e.g., "E_commerce", "Baseball")
- `target_type` (optional): `"sql"` or `"natsql"` (default: `"natsql"`)
- `spider2_mode` (required): Set to `true` for Spider2
- `tables_path` (optional): Path to `tables.json` (defaults to Spider1 path if not provided)
- `use_contents` (optional): Use database contents for schema linking (default: `true`)
- `add_fk_info` (optional): Add foreign key information (default: `true`)

#### Response Format

```json
{
  "sql": "SELECT COUNT(*) FROM orders",
  "input_sequence": "...",
  "execution_success": true,
  "error": null
}
```

**See also:** `FASTAPI_SPIDER2_README.md` and `QUICK_START_SPIDER2_API.md`

---

### Option 2: Batch Inference Pipeline

**Best for:**
- Full dataset evaluation
- Generating predictions for all questions
- Offline processing
- Production evaluation

**Advantages:**
- ðŸ“Š **Complete dataset**: Process all questions at once
- ðŸ’¾ **Persistent results**: Save predictions to file
- ðŸ”„ **Resumable**: Can skip already-completed steps
- ðŸ“ˆ **Progress tracking**: See progress for large datasets

#### Setup

1. **Run the inference script:**

```bash
# For Spider2-Lite with base model
bash scripts/inference/infer_text2natsql_spider2.sh base lite

# For Spider2-Snow
bash scripts/inference/infer_text2natsql_spider2.sh base snow

# With custom input file
bash scripts/inference/infer_text2natsql_spider2.sh base lite data/spider2/custom.jsonl
```

2. **The script performs these steps:**

   - **Step 1**: Convert Spider2 JSONL to RESDSQL format (if needed)
   - **Step 2**: Transform tables for NatSQL (if using `natsql` target)
   - **Step 3**: Preprocess dataset (schema extraction, database content sampling)
   - **Step 4**: Classify schema items (identify relevant tables/columns)
   - **Step 5**: Generate dataset for model input
   - **Step 6**: Run inference (generate SQL predictions)

3. **Predictions are saved to:**

```
predictions/{dataset_name}/resdsql_{model_scale}_natsql/pred.sql
```

Example: `predictions/spider2-lite/resdsql_base_natsql/pred.sql`

**Note:** The script is resumable - it skips steps that have already been completed.

#### Model Scales

- `base` - T5-Base (recommended for 16GB RAM)
- `large` - T5-Large (requires more memory)
- `3b` - T5-3B (requires significant memory)

**See also:** `SPIDER2_SETUP_README.md`

---

## Evaluation Methods

### Method 1: Execution-Based Evaluation with FastAPI (Recommended)

**Best for:** SQLite-only evaluation, interactive testing

This method:
1. Reads questions from `spider2-lite.jsonl`
2. Generates SQL using FastAPI server
3. Executes SQL on databases
4. Compares results with golden CSV files

#### Usage

```bash
# 1. Start FastAPI server (in one terminal)
bash start_api.sh

# 2. Run evaluation (in another terminal)
python evaluate_spider2_with_api.py \
  --spider2_jsonl /path/to/spider2-lite.jsonl \
  --gold_dir /path/to/Spider2/spider2-lite/evaluation_suite/gold \
  --output spider2_evaluation_results.json \
  --api_timeout 600 \
  --max_retries 3
```

#### Parameters

- `--spider2_jsonl`: Path to `spider2-lite.jsonl`
- `--gold_dir`: Path to Spider2 gold directory (contains `exec_result/` and `spider2lite_eval.jsonl`)
- `--api_url`: FastAPI server URL (default: `http://localhost:8000/infer`)
- `--tables_path`: Path to `tables.json` (default: `./data/spider2/converted/spider2-lite_tables.json`)
- `--database_dir`: Directory with SQLite databases (default: `./database`)
- `--target_type`: `"sql"` or `"natsql"` (default: `"natsql"`)
- `--output`: Path to save detailed results (JSON)
- `--api_timeout`: API request timeout in seconds (default: 300)
- `--max_retries`: Number of retries for failed requests (default: 2)
- `--all_databases`: Evaluate all databases, not just SQLite (default: SQLite only)

#### Output

The script prints:
- Total examples evaluated
- Execution success rate
- Correct accuracy (matches golden results)
- Error count

Detailed results are saved to JSON file with per-example information.

**See also:** `EVALUATE_SPIDER2_API_README.md`

---

### Method 2: Batch Evaluation with Pre-Generated Predictions

**Best for:** Full dataset evaluation, production runs

This method evaluates pre-generated SQL predictions from the batch inference pipeline.

#### Usage

```bash
# Evaluate predictions
python evaluate_spider2.py \
    --predictions predictions/spider2-lite/resdsql_base_natsql/pred.sql \
    --spider2_data data/spider2/spider2-lite.jsonl \
    --output evaluation_results.json
```

#### With Gold SQL (for exact match)

```bash
python evaluate_spider2.py \
    --predictions predictions/spider2-lite/resdsql_base_natsql/pred.sql \
    --gold path/to/gold_sql.txt \
    --spider2_data data/spider2/spider2-lite.jsonl \
    --output evaluation_results.json
```

**See also:** `evaluate_spider2.py` for more options

---

### Method 3: Official Spider2 Evaluation Script

**Best for:** Official submission format, publication results

Uses the official Spider2 evaluation script from the repository.

#### Setup

1. **Copy evaluation scripts from Spider2 repo:**

```bash
# Copy the official evaluation script
cp /path/to/Spider2/spider2-lite/evaluation_suite/evaluate.py \
   scripts/evaluation/

cp /path/to/Spider2/spider2-lite/evaluation_suite/evaluate_utils.py \
   scripts/evaluation/
```

2. **Prepare predictions in required format:**

Predictions should be individual `.sql` files named by `instance_id`:
```
predictions/
â”œâ”€â”€ local001.sql
â”œâ”€â”€ local002.sql
â””â”€â”€ ...
```

3. **Run evaluation:**

```bash
python scripts/evaluation/spider2_official_evaluate.py \
    --mode sql \
    --result_dir predictions/ \
    --gold_dir /path/to/Spider2/spider2-lite/evaluation_suite/gold \
    --max_workers 8
```

**See also:** `scripts/evaluation/spider2_official_evaluate.py`

---

## Troubleshooting

### Common Issues

#### 1. "FastAPI server is not running"

**Solution:**
```bash
bash start_api.sh
# Or: python api_server_refactored.py
```

Verify server is running:
```bash
curl http://localhost:8000/docs
```

#### 2. "Database file not found"

**Problem:** SQLite database path doesn't match expected structure.

**Solution:**
- Ensure databases are in `database/{db_id}/{db_id}.sqlite`
- Verify `db_id` in JSONL matches directory names
- Check that database files exist:
  ```bash
  find database/ -name "*.sqlite" | head -5
  ```

#### 3. "Tables file not found"

**Problem:** `tables.json` hasn't been generated.

**Solution:**
```bash
python utils/spider2_converter.py \
    --input data/spider2/spider2-lite.jsonl \
    --output_dir data/spider2/converted \
    --dataset_name spider2-lite
```

#### 4. "API request timed out"

**Problem:** Complex questions with large schemas take longer to process.

**Solution:**
- Increase timeout: `--api_timeout 600` (10 minutes)
- Increase retries: `--max_retries 3`
- Check API server logs for bottlenecks
- Some questions may need to be skipped (script does this by default)

#### 5. "Gold CSV not found"

**Problem:** Golden result files are missing.

**Solution:**
- Verify `--gold_dir` points to correct directory
- Ensure `exec_result/` subdirectory exists
- Check that CSV files are named correctly (by `instance_id`)

#### 6. "Schema classification stuck in loop"

**Problem:** Very large schemas cause infinite processing.

**Solution:**
- This has been fixed in `schema_item_classifier.py` with `max_iterations` limit
- Update to latest version of the script
- Large schemas will be truncated and processed with fallback values

#### 7. "Memory errors during inference"

**Problem:** Model is too large for available RAM.

**Solution:**
- Use `base` model instead of `large` or `3b`
- Reduce batch size in inference script
- Process in smaller batches

#### 8. "Cloud database connection failed"

**Problem:** BigQuery or Snowflake credentials are incorrect.

**Solution:**
- Verify environment variables are set correctly
- Check credentials file permissions
- Test connection manually:
  ```python
  from utils.database_adapters import get_database_adapter
  adapter = get_database_adapter("bigquery")
  adapter.connect({"project_id": "...", "credentials_path": "..."})
  ```

---

## File Structure

### Required Files

```
RESDSQL/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ spider2/
â”‚       â”œâ”€â”€ spider2-lite.jsonl                    # Questions dataset
â”‚       â””â”€â”€ converted/
â”‚           â””â”€â”€ spider2-lite_tables.json           # Schema info (REQUIRED)
â”‚
â”œâ”€â”€ database/                                      # SQLite databases
â”‚   â”œâ”€â”€ {db_id}/
â”‚   â”‚   â””â”€â”€ {db_id}.sqlite
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ predictions/                                   # Generated predictions
â”‚   â””â”€â”€ spider2-lite/
â”‚       â””â”€â”€ resdsql_base_natsql/
â”‚           â””â”€â”€ pred.sql
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ spider2_converter.py                      # Convert Spider2 format
â”‚   â”œâ”€â”€ database_adapters.py                      # Database adapters
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ api_server_refactored.py                      # FastAPI server
â”œâ”€â”€ evaluate_spider2.py                           # Batch evaluation
â”œâ”€â”€ evaluate_spider2_with_api.py                 # API-based evaluation
â”‚
â””â”€â”€ scripts/
    â””â”€â”€ inference/
        â””â”€â”€ infer_text2natsql_spider2.sh            # Batch inference
```

### Spider2 Repository Files (External)

```
Spider2/spider2-lite/
â”œâ”€â”€ spider2-lite.jsonl                             # Questions
â”œâ”€â”€ evaluation_suite/
â”‚   â””â”€â”€ gold/
â”‚       â”œâ”€â”€ spider2lite_eval.jsonl                 # Evaluation metadata
â”‚       â”œâ”€â”€ exec_result/                            # Golden CSV results
â”‚       â”‚   â”œâ”€â”€ local001.csv
â”‚       â”‚   â””â”€â”€ ...
â”‚       â””â”€â”€ sql/                                    # Golden SQL (optional)
â”‚           â”œâ”€â”€ local001.sql
â”‚           â””â”€â”€ ...
â””â”€â”€ resource/
    â””â”€â”€ databases/
        â””â”€â”€ sqlite/                                 # SQLite database files
            â”œâ”€â”€ E_commerce/
            â”‚   â””â”€â”€ E_commerce.sqlite
            â””â”€â”€ ...
```

---

## Quick Reference

### Common Commands

```bash
# Convert Spider2 data
python utils/spider2_converter.py \
    --input data/spider2/spider2-lite.jsonl \
    --output_dir data/spider2/converted \
    --dataset_name spider2-lite

# Start FastAPI server
bash start_api.sh

# Test single question via API
curl -X POST "http://localhost:8000/infer" \
  -H "Content-Type: application/json" \
  -d '{"question": "...", "db_id": "...", "spider2_mode": true, "tables_path": "./data/spider2/converted/spider2-lite_tables.json"}'

# Run batch inference
bash scripts/inference/infer_text2natsql_spider2.sh base lite

# Evaluate with FastAPI
python evaluate_spider2_with_api.py \
  --spider2_jsonl data/spider2/spider2-lite.jsonl \
  --gold_dir /path/to/Spider2/spider2-lite/evaluation_suite/gold \
  --output results.json

# Evaluate batch predictions
python evaluate_spider2.py \
    --predictions predictions/spider2-lite/resdsql_base_natsql/pred.sql \
    --spider2_data data/spider2/spider2-lite.jsonl \
    --output results.json
```

### Key Paths

- **Questions dataset**: `data/spider2/spider2-lite.jsonl`
- **Schema file**: `data/spider2/converted/spider2-lite_tables.json`
- **Databases**: `database/{db_id}/{db_id}.sqlite`
- **Predictions**: `predictions/spider2-lite/resdsql_base_natsql/pred.sql`
- **Gold results**: `/path/to/Spider2/spider2-lite/evaluation_suite/gold/`

---

## References

### Documentation Files

- `SPIDER2_SETUP_README.md` - Basic setup and batch inference
- `FASTAPI_SPIDER2_README.md` - FastAPI server usage
- `EVALUATE_SPIDER2_API_README.md` - API-based evaluation
- `QUICK_START_SPIDER2_API.md` - Quick start for FastAPI

### External Resources

- [Spider2 Repository](https://github.com/xlang-ai/Spider2)
- [Spider2 Paper](https://arxiv.org/abs/2411.07763)
- [Spider2 Website](https://spider2-sql.github.io/)
- [RESDSQL Repository](https://github.com/Wowchemy/RESDSQL)

### Key Scripts

- `utils/spider2_converter.py` - Convert Spider2 format
- `api_server_refactored.py` - FastAPI server
- `evaluate_spider2_with_api.py` - API-based evaluation
- `evaluate_spider2.py` - Batch evaluation
- `scripts/inference/infer_text2natsql_spider2.sh` - Batch inference pipeline

---

## Support

For issues or questions:
1. Check this guide and related README files
2. Review troubleshooting section
3. Check script help: `python script_name.py --help`
4. Review error messages and logs

---

**Last Updated:** 2024
**Maintained By:** RESDSQL Development Team
